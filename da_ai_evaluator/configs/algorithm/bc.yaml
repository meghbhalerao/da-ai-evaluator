defaults:
  - _self_


name: bc
seed: 0
log_path: ./logs/bc

data_params:
  layouts: ["cramped_room"]
  check_trajectories: false
  featurize_states: true
  data_path: datasets/
  layouts_with_data: null
  data_year: 2020

mlp_params:
  num_layers: 2
  net_arch: [256, 256]

training_params:
  epochs: 100
  validation_split: 0.15
  batch_size: 64
  learning_rate: 1e-3
  use_class_weights: false
  train_model: true

evaluation_params:
  ep_length: 400
  num_eval_episodes: 1000
  display: false
  load_from_ckpt: true
  ckpt_path: saved_files/Overcooked-Two-v0/asymmetric_advantages_tomato/1/bc_best_model.pth
  viz_traj: true
  # specify what layouts we want to evaluate our policy on
  layouts_to_eval: ["asymmetric_advantages_tomato", "counter_circuit", "cramped_corridor", "inverse_marshmallow_experiment", "marshmallow_experiment", "marshmallow_experiment_coordination", "soup_coordination", "you_shall_not_pass"]

bc_params:
  use_lstm: true
  cell_size: 256
  mdp_params:
    layout_name: "cramped_room"
    old_dynamics: false
  observation_shape: null
  featurize_type: "handcrafted" # the options are "handcrafted" or "lossless_encoding" - the handcrafted features are the one used in the original paper, and lossless ones are the raw observations in one hot style format.
  num_lstm_layers: 10
  
  env_params:
    horizon: 400
    mlam_params: null

  mdp_fn_params: {}
  action_shape: null
  # action_shape: !!python/tuple [${len:Action.ALL_ACTIONS}]  # Assuming Action.ALL_ACTIONS is defined elsewhere

# some old params possibly deprecated below 
# env: overcooked_ai
# seed: 0
# l2: 0
# total_epochs: 100
# device: auto
# batch_size: 32
# model_config:
#   param1: null